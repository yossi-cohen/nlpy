{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Forward propagation</h1>\n",
    "<p>We have a fully-connected network with <font color='red'>$L$</font> layers.\n",
    "<br>The activations of the nodes in layer $(l)$ are stored in an activations column-vector <font color='red'>$a^{(l)}$</font>, where the superscript index denote the layer. \n",
    "<br>The connections from the nodes in layer $(l-1)$ to the layer $(l)$ are stored in a weight matrix <font color='red'>$W^{(l)}$</font>, \n",
    "<br>and the biases for each node is stored in a bias column-vector <font color='red'>$b^{(l)}$</font>.\n",
    "\n",
    "<p>For a simple forward pass we have:\n",
    "    \n",
    "> ${\n",
    "    a^{(0)} = \\vec x\\\\\n",
    "    a^{(l)} = \\sigma \\left( \\vec W^{(l)} \\vec a^{(l-1)} + \\vec b^{(l)} \\right)\n",
    "}$\n",
    "\n",
    "<p>We introduce a new vector $z^{(l)} \\equiv \\left( W^{(l)} a^{(l-1)} + b^{(l)} \\right)$. \n",
    "<br>which is the activation without the application of a component-wise activation function, so that \n",
    "    $a^{(l)} = \\sigma\\left(z^{(l)}\\right)$. \n",
    "    <br>Call this value the <b>“input”</b> of a node.\n",
    "    <img src=\"images/backprop/matrix_multiplection.png\" align=\"center\" />\n",
    "\n",
    "<p>The whole network is shown below, from the input vector $x$, to the output activation vector $a^{(L)}$. \n",
    "<br>The connections leading in to a specific node is shown in colors in two layers:\n",
    "<img src=\"images/backprop/fully_connected.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Notation</h2>\n",
    "<p>\n",
    "   <font color='red'>$L$</font> - number of layers in the network\n",
    "\n",
    "   > Layers are indexed <font color='red'>$l=1,2,...,N$</font>\n",
    "   > <br>Nodes in a given layer <font color='red'>$(l)$</font> are indexed: \n",
    "        <font color='red'>$k=0,1,2,...,K-1$</font>\n",
    "   > <br>Nodes in layer <font color='red'>$(l-1)$</font> are indexed: \n",
    "        <font color='red'>$j=0,1,2,...,J-1$</font>\n",
    "<p>\n",
    "    <font color='red'>$y_k$</font> - the desiered value of node <font color='red'>$k$</font> in the output layer\n",
    "    <font color='red'>$L$</font> for a single (specific) training example.\n",
    "\n",
    "<p>\n",
    "<font color='red'>$C$</font> - the <b>cost</b> function (also loss/error function) of the network for a specific example.  \n",
    "\n",
    "> e.g. the sum of squared errors: \n",
    "> <br>$C = \\sum_i \\left(\\hat{y} - y\\right)^2$\n",
    "\n",
    "<p>\n",
    "<font color='red'>$w_{kj}^{(l)}$</font> - the weight of the connection fron node $j$ in layer ($l-1$) to node $k$ in layer $(l)$.\n",
    "\n",
    "<p>\n",
    "<font color='red'>$w_k^{(l)}$</font> - <b>weights vector</b> of node $k$ in layer $(l)$.\n",
    "\n",
    "<p>\n",
    "<font color='red'>$z_k^{(l)}$</font> - <b>input</b> for node $k$ in layer $(l)$ \n",
    "\n",
    ">$z_k^{(l)} = \\sum_k \\left(w_{kj}^{(l)} a_j^{(l-1)}\\right) + b_k^{(l)}$\n",
    "\n",
    "<p>\n",
    "<font color='red'>$\\sigma^{(l)}$</font> - the <b>activation function</b> used for layer $(l)$.\n",
    "\n",
    "<p>\n",
    "<font color='red'>$a_k^{(l)}$</font> - the <b>activation output</b> of node $k$ in layer $(l)$: \n",
    "\n",
    "> <br>$a_k^{(l)} = \\sigma\\left(z_k^{(l)}\\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deriving the error</h2>\n",
    "<p>In the figure below, we zoom at three adjacent layers anywhere in the network.\n",
    "<br>The index letter for the nodes in the layers $(l-1)$, $(l)$ and $(l+1)$ are $j$, $k$ and $m$ respectively.\n",
    "<img src=\"images/backprop/layers_jkm.png\" align=\"center\" />\n",
    "\n",
    "<p>An error function <font color='red'>$C$</font> is defined using one example from our training data.\n",
    "<br>Its derivative is calculated wrt. a single weight $w_{jk}$ in layer $(L)$.\n",
    "<p>$$\\frac{\\partial C}{\\partial w_{kj}^{(l)}}$$\n",
    "\n",
    "<p>Using the chain rule we get:\n",
    "<p>$${\n",
    "    \\frac{\\partial C}{\\partial w_{kj}^{(l)}} = \n",
    "    \\frac{\\partial C}{\\partial z_k^{(l)}} \\frac{\\partial z_k^{(l)}}{\\partial w_{kj}^{(l)}} = \n",
    "    \\frac{\\partial C}{\\partial a_k^{(l)}} \\frac{\\partial a_k^{(l)}}{\\partial z_k^{(l)}} \\frac{\\partial z_k^{(l)}}{\\partial w_{kj}^{(l)}} = \n",
    "}$$\n",
    "\n",
    "<p> Using the chain rule again:\n",
    "<br>Notice the sum in the expression below - all contributions from nodes in layer $(l+1)$ have to be accounted for since their value is affecting the end error and depend on $w_{kj}^{(l)}$ (see $a_k^{(l)}$ in the figure above).\n",
    "<p>$${\n",
    "     = \\left( \\sum_{m} \\frac{\\partial C}{\\partial z_m^{(l+1)}} \\frac{\\partial z_m^{(l+1)}}{\\partial a_k^{(l)}} \\right) \\frac{\\partial a_k^{(l)}}{\\partial z_k^{(l)}} \\frac{\\partial z_k^{(l)}}{\\partial w_{kj}^{(l)}}\\\\\n",
    "     = \\left( \\sum_{m} \\frac{\\partial C}{\\partial z_m^{(l+1)}} w_{mk}^{(l+1)} \\right) \\sigma' \\left(z_k^{(l)}\\right) a_k^{(l-1)}\n",
    "}$$\n",
    "\n",
    "<p>Notice that we only see how the error changes in response to a change in $w_{kj}$. \n",
    "<br>All other weights are held constant and their derivative wrt. $w_{kj}$ is zero.\n",
    "<br>On the other hand, the $m$ index in layer $(l+1)$ is not fixed, and the activations for nodes in that layer are changed as we change $w_{kj}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Error signal</h2>\n",
    "<p>We define the <b>“error signal”</b> of a node $k$ in layer $(l)$ as how much the total error $C$ changes \n",
    "    when the input of the node $z_k^{(l)}$ is changed:\n",
    "$$\\delta_k^{(l)} \\equiv \\frac{\\partial C} {\\partial z_k^{(l)}}$$\n",
    "\n",
    "<p>But we've already expanded this expression above:\n",
    "$${\n",
    "    \\frac{\\partial C} {\\partial z_k^{(l)}} = \n",
    "    \\left( \\sum_{m} \\frac{\\partial C}{\\partial z_m^{(l+1)}} w_{mk}^{(l+1)} \\right) \\sigma' \\left(z_k^{(l)}\\right)\n",
    "}$$\n",
    "\n",
    "<p>So we have a recursive formula for the error signals, using our definitions:\n",
    "$$\\delta_k^{(l)} = \\left( \\sum_{m} \\delta_m^{(l+1)} w_{mk}^{(l+1)} \\right)$$\n",
    "\n",
    "<p>The biases, are also “weights” and the error function should be derived with respect to them as well:\n",
    "$${\n",
    "    \\frac{\\partial C} {\\partial b_k^{(l)}} = \n",
    "    \\frac{\\partial C}{\\partial z_k^{(l)}} \\frac{\\partial z_k^{(l)}}{\\partial b_{k}^{(l)}} = \n",
    "    \\frac{\\partial C}{\\partial z_k^{(l)}} \\cdot 1 = \\delta_k^{(l)}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Propagating backwards</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
