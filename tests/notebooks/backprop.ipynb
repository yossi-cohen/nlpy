{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Backpropagation</h1>\n",
    "\n",
    "The backpropagation algorithm is typically used to train neural networks. \n",
    "<br>Each iteration of the backpropagation algorithm consists of two steps:\n",
    "1. A <font color='red'>forward propagation</font> step to compute the output of the network.\n",
    "2. A <font color='red'>backward propagation</font> step in which the error at the end of the network is propagated backward through all the neurons while updating their parameters (weights).\n",
    "\n",
    "<p>These steps are repeated for each example $(\\vec{x}, \\vec{y})$\n",
    "<br>A complete (forward-backword) run over all training examples is called an <font color='red'>epoc</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nn.png\" align=\"right\" />\n",
    "<div>\n",
    "    <h2>Notation</h2>\n",
    "    <p>\n",
    "    <font color='red'>$L$</font> - number of layers in the network\n",
    "    <br>Layers are indexed <font color='red'>$l=1,2,...,N-1,N$</font>\n",
    "    <br>Nodes in a given layer <font color='red'>$l$</font> are indexed: <font color='red'>$j=0,1,2,...,n-1$</font>\n",
    "    <br>Nodes in layer <font color='red'>$l-1$</font> are indexed: <font color='red'>$k=0,1,2,...,n-1$</font>\n",
    "    <p>\n",
    "    <font color='red'>$y_j$</font> - the desiered value of node <font color='red'>$j$</font> in the output layer\n",
    "        <font color='red'>$L$</font> for a single (specific) training example.\n",
    "    <p>\n",
    "    <font color='red'>$C_0$</font> - the <b>loss function</b> of the network for a specific example (example-0), e.g. the sum of squared errors: \n",
    "        <font color='red'>$$C_0 = \\sum_{j=0}^{n-1} \\left(\\hat{y} - y\\right)^2$$</font>.\n",
    "    <p>\n",
    "    <font color='red'>$w_{kj}^{(l)}$</font> - the weight of the connection fron node <font color='red'>$k$</font> \n",
    "        in layer <font color='red'>$l-1$</font> to node <font color='red'>$j$</font> in layer \n",
    "        <font color='red'>$l$</font>.\n",
    "    <p>\n",
    "    <font color='red'>$w_j^{(l)}$</font> - <b>weights vector</b> of node <font color='red'>$j$</font> \n",
    "        in layer <font color='red'>$l$</font>.\n",
    "    <p>\n",
    "    <font color='red'>$z_j^{(l)}$</font> - <b>input</b> for node <font color='red'>$j$</font> \n",
    "    in layer <font color='red'>$l$</font> \n",
    "        <font color='red'>$$z_j^{(l)} = {\\sum_{k=0}^{(n-1)} w_{jk}^{(l)} a_k^{(l-1)}}$$</font>\n",
    "    <p>\n",
    "    <font color='red'>$g^{(l)}$</font> - the <b>activation function</b> used for layer \n",
    "        <font color='red'>$l$</font>.\n",
    "    <p>\n",
    "    <font color='red'>$a_j^{(l)}$</font> - the <b>activation output</b> of node \n",
    "        <font color='red'>$j$</font> in layer <font color='red'>$l$</font>.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loss <font color='red'>$C_0$</font></h2>\n",
    "\n",
    "> The backward step begins with computing the loss (error) at the output layer.\n",
    "> <br>The gradient of this loss will then be propagated backwards layer by layer through the network to calculate the parameter updates.\n",
    "><br>The loss function C used here is the squared error loss function\n",
    " \n",
    "The expression <font color='red'>$\\left({a_j^{(L)} - y_j}\\right)^2$</font> \n",
    "<br>is the squared difference of the activation output and the desiered output at node <font color='red'>$j$</font> at layer <font color='red'>$L$</font>\n",
    "<br>which can be interpreted as the <b>loss of node <font color='red'>$j$</font> at layer <font color='red'>$L$</font></b>.\n",
    "<p>\n",
    "To calculate the <b>total loss</b>, we sum the loss for each node <font color='red'>$j$</font> in the output layer <font color='red'>$L$</font>\n",
    "<font color='red'>$$C_0=\\sum_{j=0}^{n-1}\\left({a_j^{(L)} - y_j}\\right)^2$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/input_for_node_j.png\" align=\"right\" />\n",
    "<h2>Input <font color='red'>$Z^{(l)}_j$</font></h2>\n",
    "\n",
    "\n",
    "The input for node <font color='red'>$j$</font> in layer <font color='red'>$(l)$</font> is the weighted sum of \n",
    "the activation outputs from the previous layer <font color='red'>$(l-1)$</font>\n",
    "<br>An individual term looks like this: <font color='red'>$$w^{(l)}_{jk} a^{(l-1)}_k$$</font>\n",
    "<p>So, the <b>input for node <font color='red'>$j$</font> in layer <font color='red'>$l$</font></b> is:\n",
    "<font color='red'>$$z_j^{(l)} = \\sum_{k=0}^{n-1} w_{jk}^{(l)} a_k^{(l-1)}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/activation_output_at_node_j.png\" align=\"right\" />\n",
    "<h2>Activation output <font color='red'>$a^{\\left(l\\right)}_j$</font></h2>\n",
    "\n",
    "\n",
    "The activation output for node <font color='red'>$j$</font> in layer <font color='red'>$(l)$</font> is the result of passing the input through an activation function <font color='red'>$g(a)$</font>\n",
    "<p>Therefore, the activation output for node <font color='red'>$j$</font> in layer <font color='red'>$(l)$</font> is expressed as:\n",
    "<p><font color='red'>$$a_j^{(l)} = g^{(l)}\\left({z_j^{(l)}}\\right) $$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Expressing the loss <font color='red'>$C_0$</font> as a composition of functions</h2>\n",
    "(in order to understand how to differentiate <font color='red'>$C_0$</font>)\n",
    "<p>Recall the definition of <font color='red'>$C_0$</font>:\n",
    "    <font color='red'>$$C_0=\\sum_{j=0}^{n-1}\\left(a_j^{(L)} - y_j\\right)^2$$</font>\n",
    "<p>So the loss of a single node <font color='red'>$j$</font> in the output layer <font color='red'>$L$</font> is:\n",
    "    <font color='red'>$C_{0_j}=\\left({a_j^{(L)} - y_j}\\right)^2$</font>\n",
    "<p>Since <font color='red'>$y_j$</font> is a constant we observe <font color='red'>$C_{0_j}$</font> is a function of <font color='red'>$a_j^L$</font> as: \n",
    "    <font color='red'>$$C_{0_j}\\left({a_j^{(L)}}\\right)$$</font>\n",
    "<p>The activation output of node <font color='red'>$j$</font> in the output layer <font color='red'>$L$</font> is a function of the input for node <font color='red'>$j$</font>.\n",
    "<br>We express this as: \n",
    "    <font color='red'>$$a_j^{(L)} = g^{(L)}\\left({z_j^{(L)}}\\right)$$</font>\n",
    "<p>The input for node <font color='red'>$j$</font> is a function of all the weights connected to node <font color='red'>$j$</font>, so we can express <font color='red'>$z_j^{(L)}$</font> as a function of <font color='red'>$w_j^{(L)}$</font> as:\n",
    "    <font color='red'>$$z_j^{(L)}\\left({w_j^{(L)}}\\right)$$</font>\n",
    "<p>Therefore\n",
    "<font color='red'>$$C_{0_j} = C_{0_j}\\left({a_j^{(L)}\\left(z_j^{(L)}\\left(w_j^{(L)}\\right)\\right)}\\right)$$</font>\n",
    "<p>So we can see that <font color='red'>$C_{0_j}$</font> is a <b>composition of functions</b>.\n",
    "<p>We know that \n",
    "    <font color='red'>$$C_0=\\sum_{j=0}^{n-1}C_{0_j}$$</font>\n",
    "<br>So the total loss of the network for a single example is also a composition of functions.\n",
    "<br>To differentiate a composition of functions we use the <b>chain rule</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/derivative_of_loss_wrt_weights.png\" align=\"right\" />\n",
    "<h2>Calculations</h2>\n",
    "<h3>Derivative of the loss wrt. the weights</h3>\n",
    "<p>We look at a single weight that connects node $2$ in layer $(L-1)$ to node $1$ in layer $(L)$\n",
    "<br>This weight is denoted as <font color='red'>$W_{12}^{(L)}$</font>\n",
    "<p>The derivative of the loss <font color='red'>$C_0$</font> with respect to this particular weight \n",
    "    <font color='red'>$W_{12}^{(L)}$</font> is denoted as\n",
    "    <font color='red'>$\\frac{\\partial C_0}{\\partial W_{12}^{(L)}}$</font>\n",
    "<p>Since <font color='red'>$C_0$</font> deplends on <font color='red'>$a_1^{(L)}$</font> \n",
    "    and <font color='red'>$a_1^{(L)}$</font> depends on <font color='red'>$z_1^{(L)}$</font> \n",
    "    and <font color='red'>$z_1^{(L)}$</font> depends on <font color='red'>$w_{12}^{(L)}$</font>,\n",
    "<br>The <b>chain rule</b> tels us that to differentiate <font color='red'>$C_0$</font> \n",
    "    wrt. <font color='red'>$w_{12}^{(L)}$</font> we take the <b>product of the derivatives</b> of the composed function.  This is expressed as \n",
    "    <font color='red'>$$\\frac{\\partial C_0}{\\partial W_{12}^{(L)}} = \n",
    "        \\left(\\frac{\\partial C_0}{\\partial a_1^{(L)}}\\right)\n",
    "        \\left(\\frac{\\partial a_1^{(L)}}{\\partial z_1^{(L)}}\\right)\n",
    "        \\left(\\frac{\\partial z_1^{(L)}}{\\partial W_{12}^{(L)}}\\right)\n",
    "    $$</font>\n",
    "<p> We'll now break down the right hand side of the equation above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The first term<font color='red'>$\\left(\\frac{\\partial C_0}{\\partial a_1^{(L)}}\\right)$</font></h3>\n",
    "<p>We know that <font color='red'>$$C_0=\\sum_{j=0}^{n-1}\\left(a_j^{(L)} - y_j\\right)^2$$</font>\n",
    "    <p>Therefore, \n",
    "    <font color='red'>$$\\frac{\\partial C_0}{\\partial a_1^{(L)}} \n",
    "    {\n",
    "        = \\frac{\\partial}{\\partial a_1^{(L)}}\\sum_{j=0}^{n-1}\\left(a_j^{(L)} - y_j\\right)^2 \\\\\n",
    "        = \\sum_{j=0}^{n-1}\\left(\\frac{\\partial}{\\partial a_1^{(L)}}\\left(a_j^{(L)} - y_j\\right)^2\\right) \\\\\n",
    "        = 2\\left(a_1^{(L)} - y_1\\right)\n",
    "    }$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The second term<font color='red'>$\\left(\\frac{\\partial a_1^{(L)}}{\\partial z_1^{(L)}}\\right)$</font></h3>\n",
    "<p>We know that for each node <font color='red'>$j$</font> in the output layer <font color='red'>$(L)$</font>\n",
    "   we have <font color='red'>$a_j^{(L)} = g^{(L)}\\left(z_j^{(L)}\\right)$</font>\n",
    "<br>Since <font color='red'>$j = 1$</font>, \n",
    "    we have <font color='red'>$a_1^{(L)} = g^{(L)}\\left(z_1^{(L)}\\right)$</font>\n",
    "<p>Therefore, \n",
    "    <font color='red'>$$\\frac{\\partial a_1^{(L)}}{\\partial z_1^{(L)}} = g^{'(L)}\\left(z_1^{(L)}\\right)$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The third term<font color='red'>$\\left(\\frac{\\partial z_1^{(L)}}{\\partial w_{12}^{(L)}}\\right)$</font></h3>\n",
    "<p>We know that for each node <font color='red'>$j$</font> in the output layer <font color='red'>$(L)$</font>\n",
    "   we have <font color='red'>$z_j^{(L)} = \\sum_{k=0}^{n-1}{\\left(w_{jk}^{(L)}a_k^{(L-1)}\\right)}$</font>\n",
    "<br>Since <font color='red'>$j = 1$</font>, \n",
    "    we have <font color='red'>$z_1^{(L)} = \\sum_{k=0}^{n-1}{\\left(w_{1k}^{(L)}a_k^{(L-1)}\\right)}$</font>\n",
    "<p>Therefore, \n",
    "<font color='red'>$$\\frac{\\partial z_1^{(L)}}{\\partial w_{12}^{(L)}} \n",
    "    = \\frac{\\partial}{\\partial w_{12}^{(L)}}\\sum_{k=0}^{n-1}{\\left(w_{1k}^{(L)}a_k^{(L-1)}\\right)}$$</font>\n",
    "Pushing the derivative inside the sum we get\n",
    "<br><font color='red'>$$\\frac{\\partial z_1^{(L)}}{\\partial w_{12}^{(L)}} \n",
    "        = \\sum_{k=0}^{n-1}{\\frac{\\partial}{\\partial w_{12}^{(L)}}\\left(w_{1k}^{(L)}a_k^{(L-1)}\\right)}$$</font>\n",
    "All items except <font color='red'>$w_{12}^{(L)}a_2^{(L-1)}$</font> get canceled and we get \n",
    "<br><font color='red'>$$\\frac{\\partial z_1^{(L)}}{\\partial w_{12}^{(L)}} = a_2^{(L-1)}$$</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combining terms</h3>\n",
    "<p>Combining all terms, we have\n",
    "    <font color='red'>$$\\frac{\\partial C_0}{\\partial W_{12}^{(L)}} { \n",
    "        = \\left(\\frac{\\partial C_0}{\\partial a_1^{(L)}}\\right)\n",
    "        \\left(\\frac{\\partial a_1^{(L)}}{\\partial z_1^{(L)}}\\right)\n",
    "        \\left(\\frac{\\partial z_1^{(L)}}{\\partial W_{12}^{(L)}}\\right) \\\\\n",
    "        = 2\\left(a_1^{(L)} - y_1\\right) \\left(g^{'(L)}\\left(z_1^{(L)}\\right)\\right) \\left(a_2^{(L-1)}\\right)\n",
    "    }$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclude</h3>\n",
    "<p>We have calculated the derivative of the loss wrt. a individual weight for one individual training example.\n",
    "<p>To calculated the derivative of the loss wrt. to this same particular weight \n",
    "    <font color='red'>$w_{12}$</font> for all <font color='red'>$n$</font> training examples, \n",
    "<br>we calculate the average of the derivatives over all <font color='red'>$n$</font> training examples.\n",
    "<p>This can be expressed as:\n",
    "    <font color='red'>$$\\frac{\\partial C}{\\partial w_{12}^{(L)}} \n",
    "    = \\frac{1}{n}\\sum_{i=0}^{n-1}{\\frac{\\partial C_i}{\\partial w_{12}^{(L)}}}$$</font>\n",
    "<p>We could then do this same process for each weight in the network, \n",
    "    to calculate the derivative of <font color='red'>$C$</font> wrt. each weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/derivative_of_the_loss_wrt_activation_outputs.png\" align=\"right\" />\n",
    "<h2>The \"back\" part in \"back propagation\"</h2>\n",
    "<h3>Derivative of the loss wrt. activation outputs</h3>\n",
    "<p>We show how to calculate the gradient of the loss wrt. each weight in the network.\n",
    "<p> Recall, the weight we choose to work with was <font color='red'>$w_{12}^{(L)}$</font> at the <b>output layer</b>, and we saw that\n",
    "    <p><font color='red'>$$\\frac{\\partial C_0}{\\partial W_{12}^{(L)}}\n",
    "            = \\left(\\frac{\\partial C_0}{\\partial a_1^{(L)}}\\right)\n",
    "            \\left(\\frac{\\partial a_1^{(L)}}{\\partial z_1^{(L)}}\\right)\n",
    "            \\left(\\frac{\\partial z_1^{(L)}}{\\partial W_{12}^{(L)}}\\right)$$</font>\n",
    "<p>Suppose we choose to work with a node that is not in the output layer like \n",
    "    <font color='red'>$w_{22}^{(L-1)}$</font>\n",
    "<p>The gradient of the loss wrt. this weight would be\n",
    "    <p><font color='red'>$$\\frac{\\partial C_0}{\\partial W_{22}^{(L-1)}}\n",
    "            = \\left(\\frac{\\partial C_0}{\\partial a_2^{(L-1)}}\\right)\n",
    "            \\left(\\frac{\\partial a_2^{(L-1)}}{\\partial z_2^{(L-1)}}\\right)\n",
    "            \\left(\\frac{\\partial z_2^{(L-1)}}{\\partial W_{22}^{(L-1)}}\\right)$$</font>\n",
    "<p>The second and the third terms on the right hand side would be calculated the exact same way as for \n",
    "    <font color='red'>$w_{12}^{(L)}$</font>\n",
    "<br>The first term on the right hand side is NOT.\n",
    "<p>The reason is that the loss is not a direct function of the activation function as in the output layer where\n",
    "    <font color='red'>$$C_0=\\sum_{j=0}^{n-1}\\left(a_j^{(L)} - y_j\\right)^2$$</font>\n",
    "<p>We need to understand how to calculate this term in order to calculate the loss wrt. any weight that is not in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Set up</h4>\n",
    "<p>We now show how to calculate the derivative of the loss wrt. any weight that is not in the output layer.\n",
    "<img src=\"images/derivative_of_the_loss_wrt_activation_outputs_2.png\" align=\"right\" />\n",
    "\n",
    "<p>The activation function for node <font color='red'>$2$</font> in layer <font color='red'>$L-1$</font> is denoted as <font color='red'>$a_2^{(L-1)}$</font>\n",
    "<p>The derivative of the loss function <font color='red'>$C_0$</font> wrt. this activation function is denoted as\n",
    "    <font color='red'>$$\\frac{\\partial C_0} {\\partial a_2^{(L-1)}}$$</font>\n",
    "\n",
    "<p>Observe that the for each node <font color='red'>$j$</font> in <font color='red'>$L$</font>, the loss <font color='red'>$C_0$</font> depends on <font color='red'>$a_j^{(L)}$</font>, and <font color='red'>$a_j^{(L)}$</font> depends on <font color='red'>$z_j^{(L)}$</font>.\n",
    "\n",
    "<p>Observe also that <font color='red'>$z_j^{(L)}$</font> depends on <b>all weights</b> connected to node <font color='red'>$j$</font> from the previous layer <font color='red'>${(L-1)}$</font> \n",
    "<br>as well as <b>all activation outputs</b> from <font color='red'>${(L-1)}$</font>.\n",
    "<br>So <font color='red'>$z_j^{(L)}$</font> depends on <font color='red'>$a_2^{(L-1)}$</font>\n",
    "\n",
    "<p>The chain rule helps us with differentiating <font color='red'>$C_0$</font> wrt. <font color='red'>$a_2^{(L-1)}$</font> by taking the product of the derivative of the composed function.\n",
    "<br>This derivative can be expressed as\n",
    "    <font color='red'>$$\\frac{\\partial C_0}{\\partial a_2^{(L-1)}} = \\sum_{j=0}^{n-1}\\left(\n",
    "        \\left(\\frac{\\partial C_0}{\\partial a_j^{(L)}}\\right)\n",
    "        \\left(\\frac{\\partial a_j^{(L)}}{\\partial z_j^{(L)}}\\right)\n",
    "        \\left(\\frac{\\partial z_j^{(L)}}{\\partial a_2^{(L-1)}}\\right)\n",
    "    \\right)$$</font>\n",
    "\n",
    "<p>This equation looks almost identical to the one obtained for the derivative of the loss wrt. a given weight.\n",
    "<br>Recall this previous derivative wrt. a given weight was expressed as\n",
    "    <font color='red'>$$\\frac{\\partial C_0}{\\partial w_{12}^{(L)}} = \n",
    "        \\left(\\frac{\\partial C_0}{\\partial a_j^{(L)}}\\right)\n",
    "        \\left(\\frac{\\partial a_j^{(L)}}{\\partial z_j^{(L)}}\\right)\n",
    "        \\left(\\frac{\\partial z_j^{(L)}}{\\partial w_{12}^{(L)}}\\right)\n",
    "    $$</font>\n",
    "\n",
    "<p>The two differences are:\n",
    "<blockquote>\n",
    "    <br>1. The summation operation.\n",
    "    <br>2. The last term on the right hand side.\n",
    "</blockquote>\n",
    "\n",
    "<p>The reason for the summation is that a change in one activation function in layer <font color='red'>$(L-1)$</font> affects each node <font color='red'>$j$</font> in layer <font color='red'>$(L)$</font> \n",
    "<br> so we need to sum up these effects. \n",
    "\n",
    "<p>Lets break the third term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The third term <font color='red'>$\\frac{\\partial z_j^{(L)}}{\\partial a_2^{(L-1)}}$</font></h3>\n",
    "<img src=\"images/derivative_of_the_loss_wrt_activation_outputs_3.png\" align=\"right\" />\n",
    "<p>We know for each node <font color='red'>$j$</font> in layer <font color='red'>$L$</font> that\n",
    "    <font color='red'>$$z_j^{(L)} = \\sum_{k=0}^{n-1}{w_{jk}^{(L)}a_k^{(L-1)}}$$</font>\n",
    "<p>Therefore, \n",
    "    <font color='red'>$$\\frac{\\partial z_j^{(L)}}{\\partial a_2^{(L-1)}} {\n",
    "        = \\frac{\\partial }{\\partial a_2^{(L-1)}}\\sum_{k=0}^{n-1}{w_{jk}^{(L)}a_k^{(L-1)}} \\\\\n",
    "        = \\sum_{k=0}^{n-1}{\\frac{\\partial }{\\partial a_2^{(L-1)}} w_{jk}^{(L)} a_k^{(L-1)}} \\\\\n",
    "        = w_{j2}^{(L)}\n",
    "    }$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combining terms</h3>\n",
    "<img src=\"images/derivative_of_the_loss_wrt_activation_outputs_4.png\" align=\"right\" />\n",
    "<p>Combining all the terms we get\n",
    "    <font color='red'>$$\\frac{\\partial C_0}{\\partial a_2^{(L-1)}} {\n",
    "        = \\sum_{j=0}^{n-1}\\left(\n",
    "            \\left(\\frac{\\partial C_0}{\\partial a_j^{(L)}}\\right)\n",
    "            \\left(\\frac{\\partial a_j^{(L)}}{\\partial z_j^{(L)}}\\right)\n",
    "            \\left(\\frac{\\partial z_j^{(L)}}{\\partial a_2^{(L-1)}}\\right)\n",
    "            \\right) \\\\\n",
    "        = \\sum_{j=0}^{n-1}{\\left(\n",
    "            2\\left(a_1^{(L)} - y_1\\right) \\left(g^{'(L)}\\left(z_1^{(L)}\\right)\\right) \\left(w_{j2}^{(L)}\\right)\n",
    "          \\right)}\n",
    "    }$$</font>\n",
    "\n",
    "<p>Now we can use this result to calculate the gradient of the loss wrt. to any weight connected \n",
    "<br>to node <font color='red'>$2$</font> in layer <font color='red'>$(L-1)$</font> like we saw for \n",
    "    <font color='red'>$w_{22}^{(L-1)}$</font> for example, with the following equation:\n",
    "<p><font color='red'>$$\\frac{\\partial C_0}{\\partial W_{22}^{(L-1)}}\n",
    "            = \\left(\\frac{\\partial C_0}{\\partial a_2^{(L-1)}}\\right)\n",
    "            \\left(\\frac{\\partial a_2^{(L-1)}}{\\partial z_2^{(L-1)}}\\right)\n",
    "            \\left(\\frac{\\partial z_2^{(L-1)}}{\\partial W_{22}^{(L-1)}}\\right)$$</font>\n",
    "\n",
    "<font color='blue'>\n",
    "<p>So essentially we need to calculate derivatives that depend on components later in the network first, \n",
    "<br>and then use these derivatives for calculating the gradient of the loss wrt. weights that come erlier in the network.\n",
    "<br><b>we achieve this by repeatedly applying the chain rule in a backword fashion</b>.\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
