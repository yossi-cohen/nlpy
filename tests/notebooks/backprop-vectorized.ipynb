{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Forward propagation</h1>\n",
    "<p>We have a fully-connected network with <font color='red'>$L$</font> layers.\n",
    "<br>The activations of the nodes in layer $(l)$ are stored in an activations column-vector <font color='red'>$a^{(l)}$</font>, where the superscript index denote the layer. \n",
    "<br>The connections from the nodes in layer $(l-1)$ to the layer $(l)$ are stored in a weight matrix <font color='red'>$W^{(l)}$</font>, \n",
    "<br>and the biases for each node is stored in a bias column-vector <font color='red'>$b^{(l)}$</font>.\n",
    "\n",
    "<p>For a simple forward pass we have:\n",
    "    \n",
    "> <font color='red'>$$a^{(0)} = x$$</font>\n",
    "> <p><font color='red'>$$a^{(l)} = \\sigma\\left(W^{(l)}a^{(l-1)} + b^{(l)}\\right)$$</font>\n",
    "\n",
    "<p>We introduce a new vector <font color='red'>$z^{(l)}$</font>. \n",
    "<br>which is the activation without the application of a component-wise activation function, so that <font color='red'>$a^{(l)} = \\sigma\\left(z^{(l)}\\right)$</font>. \n",
    "    <br>Call this value the <b>“input sum”</b> of a node.\n",
    "<img src=\"images/backprop/matrix_multiplection.png\" align=\"center\" />\n",
    "\n",
    "<p>The whole network is shown below, from the input vector $x$, to the output activation vector $a^{(L)}$. \n",
    "<br>The connections leading in to a specific node is shown in colors in two layers:\n",
    "<img src=\"images/backprop/fully_connected.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Notation</h2>\n",
    "<p>\n",
    "<font color='red'>$L$</font> - number of layers in the network\n",
    "\n",
    "> <br>Layers are indexed <font color='red'>$l=1,2,...,N-1,N$</font>\n",
    "> <br>Nodes in a given layer <font color='red'>$l$</font> are indexed: \n",
    "    <font color='red'>$j=0,1,2,...,n-1$</font>\n",
    "> <br>Nodes in layer <font color='red'>$l-1$</font> are indexed: \n",
    "    <font color='red'>$k=0,1,2,...,n-1$</font>\n",
    "<p>\n",
    "<font color='red'>$y_j$</font> - the desiered value of node <font color='red'>$j$</font> in the output layer\n",
    "    <font color='red'>$L$</font> for a single (specific) training example.\n",
    "<p>\n",
    "<font color='red'>$C$</font> - the <b>cost</b> (=loss =error) function of the network for a specific example.\n",
    "    \n",
    "> <br>e.g. the sum of squared errors: \n",
    "    $$C = \\sum_{j=0}^{n-1} \\left(\\hat{y} - y\\right)^2$$\n",
    "<p>\n",
    "<font color='red'>$w_{kj}^{(l)}$</font> - the weight of the connection fron node <font color='red'>$k$</font> \n",
    "    in layer <font color='red'>$l-1$</font> to node <font color='red'>$j$</font> in layer \n",
    "    <font color='red'>$l$</font>.\n",
    "<p>\n",
    "<font color='red'>$w_j^{(l)}$</font> - <b>weights vector</b> of node <font color='red'>$j$</font> \n",
    "    in layer <font color='red'>$l$</font>.\n",
    "\n",
    "<p>\n",
    "<font color='red'>$z_j^{(l)}$</font> - <b>input</b> for node <font color='red'>$j$</font> in layer <font color='red'>$l$</font> \n",
    "<font color='red'>$$z_j^{(l)} = \\sum_{k=0}^{(n-1)} \\left(w_{jk}^{(l)} a_k^{(l-1)}\\right) + b_j^{(l)}$$</font>\n",
    "\n",
    "<p>\n",
    "<font color='red'>$\\sigma^{(l)}$</font> - the <b>activation function</b> used for layer \n",
    "    <font color='red'>$l$</font>.\n",
    "\n",
    "<p>\n",
    "<font color='red'>$a_j^{(l)}$</font> - the <b>activation output</b> of node \n",
    "    <font color='red'>$j$</font> in layer <font color='red'>$l$</font>.\n",
    "    <font color='red'>$$a_j^{(l)} = \\sigma\\left(z_j^{(l)}\\right)$$</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deriving the error</h1>\n",
    "<p>In the figure below, we zoom at three adjacent layers anywhere in the network.\n",
    "<br>The index letter for the nodes in the layers <font color='red'>$(l-1)$, $(l)$</font> and <font color='red'>$(l+1)$</font> are <font color='red'>$j$, $k$</font> and <font color='red'>$m$</font> respectively.\n",
    "<img src=\"images/backprop/layers_jkm.png\" align=\"center\" />\n",
    "\n",
    "<p>An error function $C$ is defined using one example from our training data, and its derivative is calculated with respect to a single weight $w_{jk}$ in layer $(L)$.\n",
    "\n",
    "<p>Using the chain rule we get:\n",
    "<p><font color='red'>$$\n",
    "    \\frac{\\partial C}{\\partial w_{kj}^{(l)}} = \n",
    "    \\frac{\\partial C}{\\partial z_k^{(l)}} \\frac{\\partial z_k^{(l)}}{\\partial w_{kj}^{(l)}} = \n",
    "    \\frac{\\partial C}{\\partial a_k^{(l)}} \\frac{\\partial a_k^{(l)}}{\\partial z_k^{(l)}} \\frac{\\partial z_k^{(l)}}{\\partial w_{kj}^{(l)}} = \n",
    "    $$</font>\n",
    "\n",
    "<p> Using the chain rule again:\n",
    "<br>Notice that all contributions from the neurons in layer $(l+1)$ (indexed by $m$) have to be accounted for since their value is affecting the end error (their value is depending on the weight that we are taking the derivative with respect to).\n",
    "<p><font color='red'>$$\n",
    "     = \\sum_{m} \\left( \\frac{\\partial C}{\\partial z_m^{(l+1)}} \\frac{\\partial z_m^{(l+1)}}{\\partial a_j^{(l)}} \\right) \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}} \\frac{\\partial z_j^{(l)}}{\\partial w_{jk}^{(l)}}   $$</font>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
